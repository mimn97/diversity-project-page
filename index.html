<!DOCTYPE html>
<html>
  <head>
    <title>How Far Can We Extract Diverse Perspectives from Large Language
      Models?</title>
    <link rel="icon" type="image/x-icon" href="website/static/images/favicon.ico" />
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="website/static/css/bulma.min.css" />
    <link rel="stylesheet" href="website/static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="website/static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="website/static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="website/static/css/index.css" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="website/static/js/fontawesome.all.min.js"></script>
    <script src="website/static/js/bulma-carousel.min.js"></script>
    <script src="website/static/js/bulma-slider.min.js"></script>
    <script src="website/static/js/index.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body>
    <!-- <section class="hero is-warning">
        <div class="hero-body">
          <p class="title">
            Under Construction
          </p>
          <p class="subtitle">
            Internal preview only
          </p>
        </div>
      </section> -->
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                How Far Can We Extract Diverse Perspectives from Large Language Models?
              </h1>
              <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  <a href="" target="_blank"
                    >Anonymous Authors</a>
                  <!-- <sup>*†</sup>, -->
                </span>
                
                <br><br>
                <h4><i>Under Review by ACL 2024</i></h4>
                <!-- <span class="author-block">
                  <a href="https://karinjd.github.io/" target="_blank"
                    >Karin de Langis</a
                  ><sup>*</sup>,</span
                >
                <span class="author-block">
                  <a href="https://anmartin94.github.io/" target="_blank"
                    >Anna Martin-Boyle</a
                  ><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://sites.google.com/view/jaehyungkim" target="_blank"
                    >Jaehyung Kim</a
                  ><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://mimn97.github.io/" target="_blank"
                    >Minhwa Lee</a
                  ><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://zaemyung.github.io/" target="_blank"
                    >Zae Myung Kim</a
                  ><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://www.shirley.id/" target="_blank"
                    >Shirley Anugrah Hayati</a
                  >,
                </span>
                <span class="author-block">
                  <a target="_blank"
                    >Risako Owan</a
                  >,
                </span>
                <span class="author-block">
                  <a href="https://bin-hu.com/" target="_blank"
                    >Bin Hu</a
                  >,
                </span>
                <span class="author-block">
                  <a target="_blank"
                    >Ritik Sachin Parkar</a
                  >,
                </span>
                <span class="author-block">
                  <a href="https://kooryan.netlify.app/" target="_blank"
                    >Ryan Koo</a
                  >,
                </span>
                <span class="author-block">
                  <a target="_blank"
                    >Jong Inn Park</a
                  >,
                </span>
                <span class="author-block">
                  <a target="_blank"
                    >Aahan Tyagi</a
                  >,
                </span>
                <span class="author-block">
                  <a target="_blank"
                    >Libby Ferland</a
                  >,
                </span>
                <span class="author-block">
                  <a target="_blank"
                    >Sanjali Roy</a
                  >,
                </span>
                <span class="author-block">
                  <a target="_blank"
                    >Vincent Liu</a
                  >,
                </span>
                <span class="author-block">
                  <a href="https://dykang.github.io/" target="_blank"
                    >Dongyeop Kang</a
                  >
                </span> -->
              </div>

              <!-- <div class="is-size-5 publication-authors">
                <a class="subtitle" style="color: black !important;" href="https://minnesotanlp.github.io/">Minnesota NLP Lab<img src="website/static/logos/mnnlp.png" style="height: 1em; margin-inline: 5px;">, University of Minnesota</a>
                <span class="eql-cntrb"
                  ><small
                    ><br /><sup>*</sup>Core Contributor <sup>†</sup>Project Lead</small
                  ></span
                >
              </div> -->

              <div class="column has-text-centered">
                <div class="publication-links">
                  
                  <!-- <span class="link-block">
                    <a
                      href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark is-static"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span> -->

                  <!-- Github link -->

                  <!-- <span class="link-block">
                    <a
                      href="https://github.com/minnesotanlp/artifacts-of-llmgendata"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark is-outlined"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span> -->

                  <!-- ArXiv abstract Link -->

                  <!-- <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/<ARXIV PAPER ID>"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark is-static"
                    >
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span> -->

                  <!-- Huggingface data Link -->

                  <span class="link-block">
                    <a
                      href="https://anonymous.4open.science/r/diversity-project-page-1067/data/"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark is-outlined"
                    >
                      <span class="icon">
                        <i class="fa fa-laptop"></i>
                      </span>
                      <span>Data</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>


    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-6">
            <img
              src="website/static/images/figure1_diversity_prompting.png"
              width="500"
              class="center-image"
            />
        </div>
      </div>
    </div>

          <section class="section hero">
            <div class="container is-max-desktop">
              <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                  <h2 class="title is-3">Abstract</h2>
                  <div class="content has-text-justified">
                    <p>
                      Collecting diverse human opinions is costly and challenging. 
                      There is a recent trend in collaborative efforts between humans and Large Language Models (LLMs) for generating diverse data, offering potential scalable and efficient solutions. However, the extent of LLMs' capability to generate diverse perspectives on subjective topics remains an unexplored question. 
                      In this study, we investigate LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts. We formulate this problem as <i>diversity extraction</i> in LLMs. 
                      
                      Motivated by how humans develop their opinions through their values, we propose a criteria-based prompting technique to ground diverse opinions and measure perspective-level diversity from the generated criteria words as a novel diversity measurement. 
                      Our results show that measuring semantic diversity through sentence embeddings and distance metrics is not enough to measure perspective diversity.
                      
                      To see how far we can extract diverse perspectives from LLMs, or called <i>diversity coverage</i>, we employ a step-by-step recall prompting for generating more outputs from the model in an iterative manner. 
                      As we apply our methods to various tasks, indeed we find that LLMs can generate diverse opinions according to the degree of task subjectivity.
                    
                    </p>
                  </div>
                </div>
              </div>
            </div>
          </section>
        
    <!-- Research question start -->
    <!-- <section class="section hero is-small is-light">
      <div class="container is-max-desktop">
        <div class="content">
          <h2 class="title is-3">Research Questions</h2>
          <div class="level-set has-text-justified">
            <p><b>RQ1</b> What is the nature of SOTA artificial data? How does it compare to human data, and what artifacts does it contain?</p>
            <p><b>RQ2</b> Does training on artificial data compromise performance outcomes compared to similar human-generated data?</p>
            <p><b>RQ3</b> How specific are the artifacts of artificial data to certain types of data produced by large language models (LLMs), and how much do they apply to all types of data generated by LLMs?</p>
          </div>
        </div>
    </section> -->
    <!-- Research question end -->

    <!-- Research Contributions start -->
    <section class="section hero is-small is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-full">
            <div class="content">
              <h2 class="title is-3">Research Contributions</h2>
              <div class="level-set has-text-justified">
              <ul>
                <li>First, we propose the idea of perspective diversity for generative LLMs, unlike lexical diversity, syntactical diversity, and semantic diversity which have been main interests in previous works. 
                  We conduct various experiments to measure LLMs' ability to generate maximum perspective diversity. 
                </li>
                <li>Second,  we thus introduce a new prompting technique called criteria-based diversity prompting, as a way of extracting and grounding diverse perspectives from LLMs. 
                </li>
                <li>Finally, as it is unclear how much diversity LLMs can cover, we suggest a step-by-step approach for measuring the coverage of LLMs' diversity generation (i.e., measuring the recall for diversity prompting). 
                  We then compare this coverage between LLM's generated opinions and human-written opinions. 
                </li>
                </ul>
              </div>
            </div>
          </div>
    </section>
    <!-- Research Contributions end -->

    <!--- Methods Start -->
    <section class="hero is-small">
      <div class="hero-body">
        <div class="columns is-centered has-text-centered;">
          <h1 class="title is-3">Methods</h1>
        </div>
          <!-- <p class="subtitle has-text-centered">
            First image description.
          </p> -->
        </div> 
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <div class="item">
                <br>
                <img src="website/static/images/combined_method_1.png" alt="prompting"/>
                <!-- <p class="subtitle has-text-centered">
                  First image description.
                </p> -->
              </div>
              <div class="content has-text-justified">
                <br>
                <p>
                <ol>
                  <li> 
                    <b style="color: slateblue">Criteria-based Diversity Prompting </b> 
                    <p> 
                      Our Criteria-based Diversity Prompting is as follows (shown in Figure <b>[a]</b>):
                      <br>
                      "Given a <i>statement</i>, we prompt the LLMs to generate its <b style = "color:magenta">stance (e.g., agree or disagree)</b> and explain its <b style="color:purple">Reasons</b> with a list of <b style="color:blue">Criteria</b> that affect its perspective. ""
                    
                    <br>
                      <br>
                      Here, we consider <b style="color:blue">criteria</b> words or phrases that frame the LLM's high-level decision and generate the grounded reasons well (e.g., model values).
                    </p> 
                    <p>
                      
                    </p>
                  </li>
                  <br>
                  
                  <li>
                    <b style="color: slateblue">Step-by-Step Recall Prompting</b>
                    <p>
                      To see the LLMs' diversity coverage, we suggest a step-by-step recall prompting  (as shown in Figure <b>[b]</b> ): 
                      <br>
                      We first ask LLMs to generate one opinion ('1st Opinion') for the given statement, and we ask the models to continue generating more opinions until the requested number of opinions ('N') is reached. 
                    </p>
                    <p> Note that the first opinion is used to guide the structured format for the output since we do not do few-shot prompting for this experiment. </p>
                    </li>
                    <br>
                    <li> 
                      <b style="color: slateblue">Dataset & Models</b> 
                      <p> 
                        We collected the following datasets: (1) Social-Chem-101 (Forbes et al., 2020); (2) Change My View (CMV) (Hidey et al., 2017). 
                        For the recall prompting technique, we added the two more datasets: (3) Hate Speech (Vidgen et al., 2021); and (4) Moral Stories (Emelin et al., 2021). 
                      </p> 
                      <p>
                        Then, we assemble GPT-4, ChatGPT, and GPT-3 (text-davinci-002) as well as open-source models such as LLaMA2-70B-chat (Touvron et al., 2023) and Mistral-7B-Instruct (Jiang et al., 2023).
                      </p>
                    </li>
                    <br>
                    <li>
                    <b style="color: slateblue">Evaluation</b>
                    <p>
                      We measured the diversity in LLM-generated opinions by using the following two metrics: 
                      <ol>
                        <li>
                          <b>Semantic Diversity</b>: For each statement, we first model the generated reasons from LLMs as sentence embeddings using SentenceBERT. 
                          We then measure the cosine distance among every pair of reasons and compute the average cosine distance across all the pairs. Note that we used this metric to compare the diversity of models' generated reasons 
                          between criteria-based prompting and free-form prompting. 
                        </li>
                        <br>
                        <li>
                          <b>Perspective Diversity</b>: We prompt GPT-4 to cluster criteria words with similar meaning into one group, in order to examine the step-by-step recall prompting. 
                          A perspective diversity score for a statement is the percentage of how many generated opinions of the statement have each of their criteria not duplicated with each other. 
                          The higher the score is, the more diverse the set of generated opinions is. 
                        </li>
                      </ol>
                    </p>
                  </li>
                </ol>
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!--- Methods end -->



    <!-- Takeaway start -->
    <span id="takeaway">
      <!-- The content in this section will be replaced with the content in takeaway-mobile.html if the browser's width is less than 1024px. -->
      <section class="section hero is-small is-light">
        <div class="container is-max-desktop">
          <div class="content">
            <h2 class="title is-3 has-text-centered">Key Takeaways</h2>
            <ul>
              <li>
                <p class="subtitle">
                  GPT-4 with the criteria-based diversity prompting in an one-shot setting shows the most semantically diverse opinions about social norms and argumentative topics.
                </p>
              </li>
              <div class="hero-body">
                <div class="container">
                  <div class="item">
                    <br>
                    <img src="website/static/images/table1_semantic.png" alt="cobbler pipeline"/>
                    <!-- <p class="subtitle has-text-centered">
                      First image description.
                    </p> -->
                  </div>
                  <div class="content has-text-justified">
                    <br>
                    <p>Semantic diversity (cosine distance) results for criteria-based prompting vs. free-form prompting and LLM variants. 
                      1-criteria refers to one-shot criteria-based prompting and so on. 
                      Text for the highest diversity score within the same LLM type is made \textbf{bold}. * p< 0.05 when comparing criteria-based prompting with free-form prompting. 
                    </p>
                  </div>
                  <!-- <div id="carousel1" class="carousel results-carousel">
                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Task Labels</b> - section 5.1</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/9.png" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                                While generating <b>task labels</b>, LLMs over-represent majority opinions and do not match minority opinions well. They also show a tendency to be misleadingly confident concerning sentences with age, gender, religion, and race bias.
                                              </p>
                    </div>

                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Free-Form Text</b> - section 9.1</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/7.png" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                                In creating <b>free-form text</b>, LLMs exhibit styles that differ significantly from human approaches in diverse social contexts. Moreover, unlike human discourse, which varies widely across domains, LLM discourse patterns remain relatively consistent.
                                              </p>
                    </div>

                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Preferences</b> - section 6.1</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/15.png" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                              For elicitation of <b>preferences</b>, we find that LLM preferences are tightly coupled with standalone lexical cues, whereas human preferences appear to take a more holistic approach.
                                            </p>
                    </div>
                  </div> -->
                </div>
              </div>
              <br>
              <li>
                <p class="subtitle">
                  Task subjectivity of dataset tends to influence the capabilities of LLMs in producing the maximium number of diverse opinions. 
                </p>
              </li>
              <div class="hero-body">
                <div class="container">
                  <div id="carousel2" class="carousel results-carousel">
                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Simulation</b> - section 8.1</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/table3_criteria.png" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                        In LLM <b>simulations</b> (i.e. conversations between two LLM agents), one of the most common errors is role flipping, or the agents swapping their assigned roles. This happens most frequently when the agent becomes confused, indicating that the simulation breaks down when an agent does not understand how to respond.
                                      </p>
                    </div>

                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Instructions</b> - section 7.1</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/fig4_recall.png" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                        In <b>instruction</b> writing, LLMs may provide incorrect outputs for a written instruction rather than an output that indicates uncertainty or lack of knowledge of the answer. In downstream training, these incorrect outputs result in more hallucinations produced by instruction-tuned models.</i>
                                      </p>
                    </div>
                  </div>
                </div>
              </div>
              <li>
                <p class="subtitle">
                  LLMs are deficient in accurately mirroring human behavior for
                  particular tasks.
                </p>
              </li>
              <div class="hero-body">
                <div class="container">
                  <p class="subtitle has-text-centered"><b>Simulation</b> - section 8.1</p>
                  <div style="display: flex; justify-content: center">
                    <img src="website/static/images/19.png" style="max-height: 260px" />
                  </div>
                  <p class="subtitle is-6 has-text-centered">
                              In <b>simulations</b>, where LLM agents engage in conversations focused on problem-solving, these agents often stray from the main topic, negatively impacting task performance. This contrasts with human digressions, facilitating team building and contributing to more effective problem resolution.
                            </p>
                </div>
              </div>
              <li>
                <p class="subtitle">
                  Models trained on LLM data containing the above issues have degraded
                  performance.
                </p>
              </li>
              <div class="hero-body">
                <div class="container">
                  <div id="carousel3" class="carousel results-carousel">
                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Free-Form Text</b> - section 9.2</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/8.png" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                        We find that models trained on several types of LLM-generated data -- specifically <b>instructions</b>, <b>free-form text</b>, and <b>preferences</b> -- are at risk for degraded performance when compared to models trained on corresponding human data. In addition, these performance deficits appear to be tied to artifacts in LLM-generated data. 
                                      </p>
                    </div>
                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Task Labels</b> - section 5.2</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/12.jpg" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                      We find that models trained on several types of LLM-generated data -- specifically <b>instructions</b>, <b>free-form text</b>, and <b>preferences</b> -- are at risk for degraded performance when compared to models trained on corresponding human data. In addition, these performance deficits appear to be tied to artifacts in LLM-generated data. 
                                    </p>
                    </div>
                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Instructions</b> - section 7.2</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/14.png" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                      We find that models trained on several types of LLM-generated data -- specifically <b>instructions</b>, <b>free-form text</b>, and <b>preferences</b> -- are at risk for degraded performance when compared to models trained on corresponding human data. In addition, these performance deficits appear to be tied to artifacts in LLM-generated data. 
                                    </p>
                    </div>
                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Preferences</b> - section 6.2</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/16.png" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                    We find that models trained on several types of LLM-generated data -- specifically <b>instructions</b>, <b>free-form text</b>, and <b>preferences</b> -- are at risk for degraded performance when compared to models trained on corresponding human data. In addition, these performance deficits appear to be tied to artifacts in LLM-generated data. 
                                  </p>
                    </div>
                  </div>
                </div>
              </div>
            </ul>
          </div>
        </div>
      </section>
    </span>
    <!-- Takeaway end -->

    <!-- Limitations start -->
    <section class="section hero is-small is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full">
            <div class="content">
              <h2 class="title is-3">Limitations</h2>
              <div class="level-set has-text-justified">
                <p>Our research acknowledges various limitations in studying LLMs. We aim to offer initial insights into LLM data quality and impact, rather than conclusive findings, due to the unpredictability and complexity of LLM outputs. Our study predominantly uses existing public datasets, focusing on text data relevant to NLP, and highlights the differences between LLM and human outputs, with an emphasis on ethical considerations.</p>
                <p>However, our approach may introduce biases and limits the study's breadth. We employ human validation and qualitative analysis for assessing creativity and bias, while facing challenges in artifact analysis. Our experiments don't fully leverage the latest LLM methodologies due to resource constraints. This research, transparent in its limitations, seeks to balance practicality with relevance, providing a comprehensive understanding of the scope and implications of our findings.</p>
              </div>
            </div>
          </div>
    </section>
    <!-- Limitations end -->

    <!--BibTex citation start-->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>PENDING</code></pre>
      </div>
    </section>
    <!--BibTex citation end-->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from the <a
                  href="https://nerfies.github.io"
                  target="_blank"
                  >Nerfies</a
                > project page.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>
  

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
  </body>
</html>