<!DOCTYPE html>
<html>
  <head>
    <title>How Far Can We Extract Diverse Perspectives from Large Language
      Models? Criteria-Based Diversity Prompting!</title>
    <link rel="icon" type="image/x-icon" href="website/static/images/favicon.ico" />
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="website/static/css/bulma.min.css" />
    <link rel="stylesheet" href="website/static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="website/static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="website/static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="website/static/css/index.css" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="website/static/js/fontawesome.all.min.js"></script>
    <script src="website/static/js/bulma-carousel.min.js"></script>
    <script src="website/static/js/bulma-slider.min.js"></script>
    <script src="website/static/js/index.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body>
    <!-- <section class="hero is-warning">
        <div class="hero-body">
          <p class="title">
            Under Construction
          </p>
          <p class="subtitle">
            Internal preview only
          </p>
        </div>
      </section> -->
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!
              </h1>
              <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  <a href="" target="_blank"
                    >Anonymous Authors</a>
                  <!-- <sup>*†</sup>, -->
                </span>
                
                <br><br>
                <h4><i>Under Review by ACL 2024</i></h4>
                <!-- <span class="author-block">
                  <a href="https://karinjd.github.io/" target="_blank"
                    >Karin de Langis</a
                  ><sup>*</sup>,</span
                >
                <span class="author-block">
                  <a href="https://anmartin94.github.io/" target="_blank"
                    >Anna Martin-Boyle</a
                  ><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://sites.google.com/view/jaehyungkim" target="_blank"
                    >Jaehyung Kim</a
                  ><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://mimn97.github.io/" target="_blank"
                    >Minhwa Lee</a
                  ><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://zaemyung.github.io/" target="_blank"
                    >Zae Myung Kim</a
                  ><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://www.shirley.id/" target="_blank"
                    >Shirley Anugrah Hayati</a
                  >,
                </span>
                <span class="author-block">
                  <a target="_blank"
                    >Risako Owan</a
                  >,
                </span>
                <span class="author-block">
                  <a href="https://bin-hu.com/" target="_blank"
                    >Bin Hu</a
                  >,
                </span>
                <span class="author-block">
                  <a target="_blank"
                    >Ritik Sachin Parkar</a
                  >,
                </span>
                <span class="author-block">
                  <a href="https://kooryan.netlify.app/" target="_blank"
                    >Ryan Koo</a
                  >,
                </span>
                <span class="author-block">
                  <a target="_blank"
                    >Jong Inn Park</a
                  >,
                </span>
                <span class="author-block">
                  <a target="_blank"
                    >Aahan Tyagi</a
                  >,
                </span>
                <span class="author-block">
                  <a target="_blank"
                    >Libby Ferland</a
                  >,
                </span>
                <span class="author-block">
                  <a target="_blank"
                    >Sanjali Roy</a
                  >,
                </span>
                <span class="author-block">
                  <a target="_blank"
                    >Vincent Liu</a
                  >,
                </span>
                <span class="author-block">
                  <a href="https://dykang.github.io/" target="_blank"
                    >Dongyeop Kang</a
                  >
                </span> -->
              </div>

              <!-- <div class="is-size-5 publication-authors">
                <a class="subtitle" style="color: black !important;" href="https://minnesotanlp.github.io/">Minnesota NLP Lab<img src="website/static/logos/mnnlp.png" style="height: 1em; margin-inline: 5px;">, University of Minnesota</a>
                <span class="eql-cntrb"
                  ><small
                    ><br /><sup>*</sup>Core Contributor <sup>†</sup>Project Lead</small
                  ></span
                >
              </div> -->

              <div class="column has-text-centered">
                <div class="publication-links">
                  
                  <!-- <span class="link-block">
                    <a
                      href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark is-static"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span> -->

                  <!-- Github link -->

                  <!-- <span class="link-block">
                    <a
                      href="https://github.com/minnesotanlp/artifacts-of-llmgendata"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark is-outlined"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span> -->

                  <!-- ArXiv abstract Link -->

                  <!-- <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/<ARXIV PAPER ID>"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark is-static"
                    >
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span> -->

                  <!-- Huggingface data Link -->

                  <!-- <span class="link-block">
                    <a
                      href="https://huggingface.co/datasets/minnesotanlp/LLM-Artifacts"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark is-outlined"
                    >
                      <span class="icon">
                        <img src="static/logos/hf-logo.svg" width="25" height="25"></img>
                      </span>
                      <span>Data</span>
                    </a>
                  </span> -->
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>


          <div class="container is-max-desktop">
            <div class="columns is-centered">
              <div class="image-container">
                <img src="website/static/images/figure1_diversity_prompting.png" alt="Image 1">
                <img src="website/static/images/figure2_our_task.png" alt="Image 2">
              </div>
            </div>
          </div>

          <section class="section hero">
            <div class="container is-max-desktop">
              <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                  <h2 class="title is-3">Abstract</h2>
                  <div class="content has-text-justified">
                    <p>
                      Collecting diverse human opinions is costly and challenging. 
                      There is a recent trend in collaborative efforts between humans and Large Language Models (LLMs) for generating diverse data, offering potential scalable and efficient solutions. However, the extent of LLMs' capability to generate diverse perspectives on subjective topics remains an unexplored question. 
                      In this study, we investigate LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts. We formulate this problem as <i>diversity extraction</i> in LLMs. 
                      
                      Motivated by how humans develop their opinions through their values, we propose a criteria-based prompting technique to ground diverse opinions and measure perspective-level diversity from the generated criteria words as a novel diversity measurement. 
                      Our results show that measuring semantic diversity through sentence embeddings and distance metrics is not enough to measure perspective diversity.
                      
                      To see how far we can extract diverse perspectives from LLMs, or called <i>diversity coverage</i>, we employ a step-by-step recall prompting for generating more outputs from the model in an iterative manner. 
                      As we apply our methods to various tasks, indeed we find that LLMs can generate diverse opinions according to the degree of task subjectivity.
                    
                    </p>
                  </div>
                </div>
              </div>
            </div>
          </section>
        
    <!-- Research question start -->
    <!-- <section class="section hero is-small is-light">
      <div class="container is-max-desktop">
        <div class="content">
          <h2 class="title is-3">Research Questions</h2>
          <div class="level-set has-text-justified">
            <p><b>RQ1</b> What is the nature of SOTA artificial data? How does it compare to human data, and what artifacts does it contain?</p>
            <p><b>RQ2</b> Does training on artificial data compromise performance outcomes compared to similar human-generated data?</p>
            <p><b>RQ3</b> How specific are the artifacts of artificial data to certain types of data produced by large language models (LLMs), and how much do they apply to all types of data generated by LLMs?</p>
          </div>
        </div>
    </section> -->
    <!-- Research question end -->

    <!-- Research Contributions start -->
    <section class="section hero is-small is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full">
            <div class="content">
              <h2 class="title is-3">Research Contributions</h2>
              <div class="level-set has-text-justified">
              <ul>
                <li>First, we propose the idea of perspective diversity for generative LLMs, unlike lexical diversity, syntactical diversity, and semantic diversity which have been main interests in previous works. 
                  We conduct various experiments to measure LLMs' ability to generate maximum perspective diversity. 
                </li>
                <li>Second,  we thus introduce a new prompting technique called criteria-based diversity prompting, as a way of extracting and grounding diverse perspectives from LLMs. 
                </li>
                <li>Finally, as it is unclear how much diversity LLMs can cover, we suggest a step-by-step approach for measuring the coverage of LLMs' diversity generation (i.e., measuring the recall for diversity prompting). 
                  We then compare this coverage between LLM's generated opinions and human-written opinions. 
                </li>
                </ul>
              </div>
            </div>
          </div>
    </section>
    <!-- Research Contributions end -->

    <!-- Takeaway start -->
    <span id="takeaway">
      <!-- The content in this section will be replaced with the content in takeaway-mobile.html if the browser's width is less than 1024px. -->
      <section class="section hero is-small">
        <div class="container is-max-desktop">
          <div class="content">
            <h2 class="title is-3">Key Takeaways</h2>
            <ul>
              <li>
                <p class="subtitle">
                  LLMs demonstrate a subpar understanding of complex human opinions
                  and interactions.
                </p>
              </li>
              <div class="hero-body">
                <div class="container">
                  <div id="carousel1" class="carousel results-carousel">
                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Task Labels</b> - section 5.1</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/9.png" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                                While generating <b>task labels</b>, LLMs over-represent majority opinions and do not match minority opinions well. They also show a tendency to be misleadingly confident concerning sentences with age, gender, religion, and race bias.
                                              </p>
                    </div>

                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Free-Form Text</b> - section 9.1</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/7.png" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                                In creating <b>free-form text</b>, LLMs exhibit styles that differ significantly from human approaches in diverse social contexts. Moreover, unlike human discourse, which varies widely across domains, LLM discourse patterns remain relatively consistent.
                                              </p>
                    </div>

                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Preferences</b> - section 6.1</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/15.png" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                              For elicitation of <b>preferences</b>, we find that LLM preferences are tightly coupled with standalone lexical cues, whereas human preferences appear to take a more holistic approach.
                                            </p>
                    </div>
                  </div>
                </div>
              </div>
              <li>
                <p class="subtitle">
                  LLMs struggle to respond effectively when faced with unknown or
                  unfamiliar situations.
                </p>
              </li>
              <div class="hero-body">
                <div class="container">
                  <div id="carousel2" class="carousel results-carousel">
                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Simulation</b> - section 8.1</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/17.png" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                        In LLM <b>simulations</b> (i.e. conversations between two LLM agents), one of the most common errors is role flipping, or the agents swapping their assigned roles. This happens most frequently when the agent becomes confused, indicating that the simulation breaks down when an agent does not understand how to respond.
                                      </p>
                    </div>

                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Instructions</b> - section 7.1</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/13.png" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                        In <b>instruction</b> writing, LLMs may provide incorrect outputs for a written instruction rather than an output that indicates uncertainty or lack of knowledge of the answer. In downstream training, these incorrect outputs result in more hallucinations produced by instruction-tuned models.</i>
                                      </p>
                    </div>
                  </div>
                </div>
              </div>
              <li>
                <p class="subtitle">
                  LLMs are deficient in accurately mirroring human behavior for
                  particular tasks.
                </p>
              </li>
              <div class="hero-body">
                <div class="container">
                  <p class="subtitle has-text-centered"><b>Simulation</b> - section 8.1</p>
                  <div style="display: flex; justify-content: center">
                    <img src="website/static/images/19.png" style="max-height: 260px" />
                  </div>
                  <p class="subtitle is-6 has-text-centered">
                              In <b>simulations</b>, where LLM agents engage in conversations focused on problem-solving, these agents often stray from the main topic, negatively impacting task performance. This contrasts with human digressions, facilitating team building and contributing to more effective problem resolution.
                            </p>
                </div>
              </div>
              <li>
                <p class="subtitle">
                  Models trained on LLM data containing the above issues have degraded
                  performance.
                </p>
              </li>
              <div class="hero-body">
                <div class="container">
                  <div id="carousel3" class="carousel results-carousel">
                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Free-Form Text</b> - section 9.2</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/8.png" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                        We find that models trained on several types of LLM-generated data -- specifically <b>instructions</b>, <b>free-form text</b>, and <b>preferences</b> -- are at risk for degraded performance when compared to models trained on corresponding human data. In addition, these performance deficits appear to be tied to artifacts in LLM-generated data. 
                                      </p>
                    </div>
                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Task Labels</b> - section 5.2</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/12.jpg" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                      We find that models trained on several types of LLM-generated data -- specifically <b>instructions</b>, <b>free-form text</b>, and <b>preferences</b> -- are at risk for degraded performance when compared to models trained on corresponding human data. In addition, these performance deficits appear to be tied to artifacts in LLM-generated data. 
                                    </p>
                    </div>
                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Instructions</b> - section 7.2</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/14.png" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                      We find that models trained on several types of LLM-generated data -- specifically <b>instructions</b>, <b>free-form text</b>, and <b>preferences</b> -- are at risk for degraded performance when compared to models trained on corresponding human data. In addition, these performance deficits appear to be tied to artifacts in LLM-generated data. 
                                    </p>
                    </div>
                    <div class="item">
                      <p class="subtitle has-text-centered"><b>Preferences</b> - section 6.2</p>
                      <div style="display: flex; justify-content: center">
                        <img src="website/static/images/16.png" style="max-height: 260px" />
                      </div>
                      <p class="subtitle is-6 has-text-centered">
                                    We find that models trained on several types of LLM-generated data -- specifically <b>instructions</b>, <b>free-form text</b>, and <b>preferences</b> -- are at risk for degraded performance when compared to models trained on corresponding human data. In addition, these performance deficits appear to be tied to artifacts in LLM-generated data. 
                                  </p>
                    </div>
                  </div>
                </div>
              </div>
            </ul>
          </div>
        </div>
      </section>
    </span>
    <!-- Takeaway end -->

    <!-- Limitations start -->
    <section class="section hero is-small is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full">
            <div class="content">
              <h2 class="title is-3">Limitations</h2>
              <div class="level-set has-text-justified">
                <p>Our research acknowledges various limitations in studying LLMs. We aim to offer initial insights into LLM data quality and impact, rather than conclusive findings, due to the unpredictability and complexity of LLM outputs. Our study predominantly uses existing public datasets, focusing on text data relevant to NLP, and highlights the differences between LLM and human outputs, with an emphasis on ethical considerations.</p>
                <p>However, our approach may introduce biases and limits the study's breadth. We employ human validation and qualitative analysis for assessing creativity and bias, while facing challenges in artifact analysis. Our experiments don't fully leverage the latest LLM methodologies due to resource constraints. This research, transparent in its limitations, seeks to balance practicality with relevance, providing a comprehensive understanding of the scope and implications of our findings.</p>
              </div>
            </div>
          </div>
    </section>
    <!-- Limitations end -->

    <!--BibTex citation start-->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>PENDING</code></pre>
      </div>
    </section>
    <!--BibTex citation end-->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from the <a
                  href="https://nerfies.github.io"
                  target="_blank"
                  >Nerfies</a
                > project page.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>
    
    <script>
      function loadHtmlWithScripts(targetElemId, filePath) {
          return fetch(filePath)
              .then((response) => response.text())
              .then((data) => {
                  const targetElem = document.getElementById(targetElemId);
                  targetElem.innerHTML = data;

                  const scripts = targetElem.querySelectorAll("script");
                  scripts.forEach((script) => {
                      const newScript = document.createElement("script");
                      if (script.src) {
                          newScript.src = script.src;
                      } else {
                          newScript.textContent = script.textContent;
                      }
                      document.body.appendChild(newScript);
                      newScript.remove();
                  });
              });
      }
      function loadTakeaway() {
          if (window.innerWidth < 1024) {
              loadHtmlWithScripts('takeaway', 'takeaway-mobile.html');
          }
      }

      // loadTakeaway();

    </script>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
  </body>
</html>